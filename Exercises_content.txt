
02450 2022 DTU -> VIR CTU Exercises conetents:
ex0.4.3
ex0.4.4
ex0.4.5
ex0.5.1 basic plot and label
ex0.5.2 plot from two sensors, labeled, grid

ex1.5.1 import data set
ex1.5.2 
ex1.5.3
ex1.5.4
ex1.5.5 Handling NaN and outliers

ex2.1.1 
ex2.1.2 Scatter plot of all attributes
ex2.1.3 PCA Variance, how many components needed
ex2.1.4 PCA scatter plot
ex2.1.5 PCA component coeffitients
ex2.1.6 PCA a lot of graphs
ex2.2.1 
ex2.2.2 PCA on MNIST
ex2.3.1 PCA components needed for error rate
 
ex3.1.2 Read from txt file and finding back of words automatically
ex3.1.3 Discarding „stop words“ like „a“, „is“ carrying no information
ex3.1.4 stemming: using derived words like "computable, computing, computed" as "comput"
ex3.1.5 Using query (some other sentence containing some of attr. Names) and measuring similarity with already seen sentences by cos similarity
ex3.2.1 Computing mean, SMC, Jaccard, Cosine similarity
ex3.3.1 Using SMC, Jaccard, ExtendedJAccard, Cosine and Correlation similarities on faces
ex3.3.2 Scaling and translating SMC, Cos... -> cos scaling is unvariant, but not for translation
 
ex4.1.1 Generating and plotting Gaussian distribution
ex4.1.2 using generated points to compute mean & standard deviation
ex4.1.3 Plotting random Gauss distribution with true Gauss distribution
ex4.1.4 Multivariable - generating samples with Gauss 2D distribution
ex4.1.5 Plotting scatter and 2D histogram of random samples of Gauss distribution
ex4.1.6 Estimating mean and std deviation on MNIST
ex4.1.7 Instead of using 1D 256 computed values of numbers, we can used 1x256D case
ex4.2.1 SUMMARY FOR PROJECT 1: Loading data
ex4.2.2     histograms 
ex4.2.3     box plot 
ex4.2.4     box plot for each attribute
ex4.2.5     scatter plots of each combination of two attributes
ex4.2.6     3D scatter plot of three attributes
ex4.2.7     plot the data matrix as an image
ex4.3.1     visualising Wine dataset from matlab
ex4.3.2     scatter plots of attributes: are there any suitable for classification?
 
ex5.1.1 importing data set for decision tree
ex5.1.2 creating and visualizing decision tree
ex5.1.3 decision tree, instead of computing inpurity by Gini, we use sth else
ex5.1.4 Full prediction classifier decision tree
ex5.1.5 removing outliers from wine
ex5.1.6 Using removed outliers and min_samples_split - smaller number means bigger width of tree
ex5.1.7 
ex5.2.1 LINEAR regression with noise (e)
ex5.2.2 lineary regress. estimate w0, w1
ex5.2.3 polynomial fitting with different K order polynomial and overfitting
ex5.2.4 predict alcohol in wine dataset
ex5.2.5 regression with more variables and transforms and residual plots
    - residuals are useful 22.00 exe walkthrough
ex5.2.6 LOGISTIC regression with already removed outliers
 
ex6.1.1 CROSS-VALIDAION HOLD-OUT decision tree prunning evaluation on wine dataset with removed outliers
    - each time we get a bit different data. What prune to choose? When the error rate doesnt go down anymore
    - train goes down = nice fit, but test goes up at some point = overfitting
    - apart from tree depth we can tune for example impurity method
ex6.1.2 10-fold cross validation - try adjusting X times -fold and see results
ex6.2.1 2-level cross validation and sequential feature selection on LINEAR REGRESSION
    - we are not only validating but chosing the best method
    - selecting best features, outer loop choosing the best model
    - feature selection: chosing only some best attributes forward/backwards (doing forward here)
    - plots show the outer loop. We stop iteration when error doesnt go down anymore (USE FOR REPORT)
         - on cubes plot we chose one attribute and add another one in next iteration until error doesnt go down anymore
    - cubes plot with Crossfold on X axes: inner loop. Will show us which attributes are very important
    - text output: using all attributes (no selection). Lower value is worse. Test error is more important
ex6.3.1 KNN clustering classification - try adjusting K 
    - confusion matrix: if everything classified correctly, wil be on diagonal only
    - third dataset doesnt work well
    - same scales on axis is very important for good classification
    - LOW K: low bias, high variance, LARGE K: high bias, low variance. Good for labels with noise.
ex6.3.2 cross validation with leave-one-out and number of neighbours automatic estimation for K
    - what method to chose: do we want good error rate or nice visualization? KNN and trees are intuitive for visualising
    - Trees need more parameters to tune than K in KNN
    - Regression is nice for propability of classes, we dont get that in KNN or trees
ex7.1.1 STATISTICS FOR CLASS: Compare 3 models for KNN by cross-validation and compute their accuracy
ex7.1.2 Estimate Theta point by Jeffrey interval for one model
ex7.1.4 Compares two models with confidence intervale and compute p-value for hypotesis analysis
ex7.2.1 STATISTICS FOR REGRESSION:Compute confidence interval, p-value, split into ONE-HOT
ex7.3.1 STATISTICS that take into account new dataset for testing (as previous dont have to work on new dataset)
ex7.4.3  preps for NAIVE BAYES
ex7.4.4  compute NAIVE BAYES and error rate
 
ex8.1.1 Two layer CV: Regularization, standardization, cross-validation of dataset for linear regression
ex8.1.2 The same for logistic regression
    - Hold-out method CV
ex8.2.2 ANN with 1 input layer, 1 hidden, 1 output
    - if activation function is linear, it would at best be linear regression, else (ReLU/tanh) its more of a logistic regression
        but with growing number of neurons it can be a lot more flexible.
    - ANN dont have to output the optimal output, so we train them several times
    - ANN can be used for both classification and regression
    - there is BCELoss loss function, that is only good for binary data!
ex8.2.5 ANN with one unit
ex8.2.6 ANN for Regression
ex8.3.1 
ex8.3.2 
ex8.3.3 
 
ex9.1.1 bagging & ensemble, not helpful much for linear classifiers
ex9.1.2 Boosting - applying loss function. Complicated decision boundary, almost perfect with L=500
    - then bagging on nonlinear classification
ex9.2.1 logistic regression, ROC and confusion matrix
ex9.2.2 ilustration of ROC with bad classifier
ex9.2.3 
 
ex10.1.1 
ex10.1.3 
ex10.1.5 
ex10.2.1 
 
ex11.1.1 
ex11.1.5 
ex11.2.1 
ex11.2.2 
ex11.2.3 
ex11.3.1 
ex11.3.2 
ex11.4.1 
ex12.1.3 
ex12.1.4 
ex12.1.5 
ex12.1.6 